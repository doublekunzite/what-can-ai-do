<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- CRITICAL: Theme script must be FIRST to prevent flash -->
    <script>
        // Apply theme immediately before page renders
        (function() {
            const savedTheme = localStorage.getItem('theme');
            const systemPrefersLight = window.matchMedia && window.matchMedia('(prefers-color-scheme: light)').matches;
            
            // Default to dark mode unless user explicitly chose light
            if (savedTheme === 'light' || (!savedTheme && systemPrefersLight)) {
                document.documentElement.style.backgroundColor = '#1a1a1a';
            }
        })();
    </script>
    
    <title>FAQ - What can AI do?</title>
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22  viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>" type="image/svg+xml">
    <link rel="stylesheet" href="css/style.css?v=2">
</head>
<body>
    <button id="theme-toggle" class="theme-toggle">üåô Dark</button>

    <main>
	        <div id="nav-container"></div>

        <h1>Common Concerns</h1>
        <p class="subtitle">Addressing the most frequent objections to AI with nuance, not hype.</p>
        

        <!-- Isn't AI bad? Dropdown -->
        <details class="main-dropdown">
            <summary class="dropdown-header">
                <h2>Isn't AI bad?</h2>
            </summary>
            <div class="dropdown-content">
                <p>AI is a foundational technology that has and will continue to eliminate barriers and revolutionize modern life. Classing it in a simple good/bad moral framework is similar to asking whether the Internet is bad, or whether automobiles are bad. Attempting to boycott foundational, ubiquitous technologies‚Äîor shaming people for using them‚Äîisn't sensible, but rather based on idealism and ignorance or neglect of material conditions.</p>
                
                <p>A better question would be, "How can we maximize the benefit of this technology to society while minimizing the risks and harms?" Countries like China are already viewing AI through this progressive lens and integrating it into their education systems‚Äîwhile also keeping domestic AI companies tightly regulated.</p>
				
   
    <div class="ai-output-box">
        <h4>Kimi K2 output:</h4>
        <p>The figure you half-remembered is the <strong>two-bar chart</strong> that appears on p. 4 of the April-2024 Stanford SPARQ brief <em>‚ÄúHow Culture Shapes What People Want From AI‚Äù</em>.</p>
        <p>It breaks <strong>‚Äúimportance ratings‚Äù</strong> (1 = not at all, 7 = extremely) for two opposite goals:</p>
        <ul>
            <li><strong>‚ÄúHaving control over AI‚Äù</strong></li>
            <li><strong>‚ÄúConnecting with AI‚Äù</strong></li>
        </ul>
        <p>across two cultural groups:</p>
        <ul>
            <li><strong>European-Americans</strong> (a proxy for the U.S. individualist baseline)</li>
            <li><strong>Chinese</strong> (a proxy for an East-Asian collectivist baseline)</li>
        </ul>
        <h3>1. What the bars actually show</h3>
        <table>
            <tr>
                <th>Statement</th>
                <th>European-Americans</th>
                <th>Chinese</th>
            </tr>
            <tr>
                <td><strong>‚ÄúIt is important that I have control over AI‚Äù</strong></td>
                <td><strong>‚âà 5.7 / 7</strong></td>
                <td><strong>‚âà 4.9 / 7</strong></td>
            </tr>
            <tr>
                <td><strong>‚ÄúIt is important that I can connect with AI‚Äù</strong></td>
                <td><strong>‚âà 3.4 / 7</strong></td>
                <td><strong>‚âà 4.6 / 7</strong></td>
            </tr>
        </table>
        <p>So the <strong>gap is almost a full scale-point in opposite directions</strong>:</p>
        <ul>
            <li>Individualists want <strong>personal agency</strong> first, social warmth second.</li>
            <li>Collectivists want <strong>relational harmony</strong> first, personal control second.</li>
        </ul>
        <h3>2. Why this pattern repeats in other data</h3>
        <p>The same cultural tilt shows up in bigger, independently-run surveys:</p>
        <ul>
            <li><strong>Ipsos ‚ÄúGlobal Advisor‚Äù 2023</strong> ‚Äì 66% of Chinese respondents say <em>‚ÄúAI will improve my life‚Äù</em> versus 39% of Americans.</li>
            <li><strong>CDI 2024 enterprise panel</strong> ‚Äì Only 24% of Chinese employees voice <em>‚Äúnegative feelings about AI at work‚Äù</em> compared with 51% of Americans and 55-65% of major EU national samples.</li>
        </ul>
        <p>In other words, collectivist publics not only <strong>rate benevolent-connection items higher</strong>, they also <strong>rate risk/control items lower</strong>, producing the net <em>‚Äúmore favourable view‚Äù</em> you saw.</p>
        <h3>3. The mechanism: collectivist cognitive filter</h3>
        <p>Hofstede-type research summarised in the LinkedIn think-piece <em>‚ÄúAI & Cultural Orientations‚Äù</em> explains the psychological chain:</p>
        <ol>
            <li><strong>Core value</strong>: <em>group welfare precedes individual autonomy</em>.</li>
            <li><strong>Trust default</strong>: institutions (state, tech firms) are seen as extensions of the in-group; therefore algorithmic delegation is not automatically coded as <em>‚Äúloss of my freedom‚Äù</em>.</li>
            <li><strong>Benefit frame</strong>: AI is read through <strong>communal pay-offs</strong> ‚Äì safer cities, smoother traffic, elder-care robots ‚Äì so the technology is <strong>morally congruent</strong> with existing norms.</li>
        </ol>
        <p>By contrast, in individualist cultures the <strong>first filter is ‚ÄúDoes it threaten my self-determination or privacy?‚Äù</strong>; if the answer is <em>‚Äúpossibly‚Äù</em>, affect turns negative early and colours later judgements.</p>
        <h3>4. Practical fallout already visible</h3>
        <ul>
            <li><strong>Product design</strong>: Mainland apps (WeChat, Douyin) bake AI features straight into group feeds or collective mini-games; Western launches (e.g., Chat-GPT) foreground <em>‚Äúyour personal assistant‚Äù</em> and emphasise opt-out buttons.</li>
            <li><strong>Policy rhetoric</strong>: China‚Äôs 2025 <em>‚ÄúAI for Social Good‚Äù</em> road-map literally lists <em>‚Äúharmonious society‚Äù</em> as the top KPI; EU documents open with <em>‚Äúfundamental-rights impact‚Äù</em>.</li>
            <li><strong>Adoption curves</strong>: 32% of Chinese firms were already using AI in 2018 vs 22% in the U.S. and 18% in the EU ‚Äì a rank order that mirrors the affect gap.</li>
        </ul>
        <h3>5. Limitations & future unknowns</h3>
        <ul>
            <li><strong>Intra-culture variance</strong>: Young, urban Chinese tech-workers can be <strong>more privacy-sensitive than their parents</strong>; U.S. military or health-care professionals can be <strong>highly group-oriented</strong>. Culture is probabilistic, not deterministic.</li>
            <li><strong>Issue specificity</strong>: Ask about <strong>AI in hiring or credit-scoring</strong> and even collectivist respondents suddenly want <strong>individual appeal rights</strong>; ask about <strong>AI writing your wedding speech</strong> and individualists love the creative shortcut.</li>
            <li><strong>Generative-AI shock</strong>: 2023-24 data may already be ageing; mass exposure to human-like chatbots could be eroding the <em>‚Äúcontrol‚Äù</em> premium in the West and raising <em>‚Äúauthenticity‚Äù</em> concerns in the East.</li>
        </ul>
        <p>Still, the <strong>collectivist = more AI-friendly</strong> snapshot you recall is <strong>real, replicated, and theory-consistent</strong> ‚Äì and the Stanford SPARQ bar chart is the cleanest visual distillation of it.</p>
        <p><strong>Source:</strong> <a href="https://sparq.stanford.edu/sites/g/files/sbiybj19021/files/media/file/culture-ai.pdf" target="_blank">How Culture Shapes What People Want From AI (Stanford SPARQ)</a></p>
    </div>
				
			</div>
        </details>

        <!-- Won't AI make us stupid? Dropdown -->
        <details class="main-dropdown">
            <summary class="dropdown-header">
                <h2>Won't AI make us stupid?</h2>
            </summary>
            <div class="dropdown-content">
                <p>There are certainly risks of AI use compromising the development of certain skills. AI LLMs can also be <a href="https://doublekunzite.github.io/what-can-ai-do/compare.html#model-agreeability">excessively agreeable</a>. The need for critical thinking skills and study of real world material conditions is only heightened by the existence of this technology. Becoming a savvy and cautious user of AI, capable of auditing and challenging AI output, will become an increasingly valuable skill in its own right.</p>
                
                <!-- Kimi K2 output - special formatting -->
                <div class="ai-output-box">
                    <h4>Kimi K2 output [28 November 2025]:</h4>
                    <p>"AI is making us stupid"</p>
                    <p>Valid concern. Over-reliance on calculators hurt mental math skills; GPS reduced spatial memory. AI could similarly atrophy certain cognitive abilities.</p>
                    <p><strong>The research:</strong> Studies show students who overuse AI perform worse on critical thinking tasks. But those who use AI strategically‚Äîenhancing rather than replacing effort‚Äîperform better.</p>
                    <p><strong>The parallel:</strong> Like power tools: they make you more capable if you understand the craft, but dangerous if you skip learning fundamentals.</p>
                    <p><strong>Your move:</strong> Practice skills without AI regularly. Use it to check your work, not do your thinking. Always understand <em>why</em> the AI suggested something.</p>
                </div>
				
				<p>AI is useful for bridging the gap on skills you don't care to learn, or repetitive tasks you'd rather skip. For example, I didn't care to learn in-depth coding skills to create this website, so I was happy to give the AI a plain language description of what I wanted and let it do the bulk of the coding work. <strong>I ended up learning some basic coding skills along the way that I'd never have picked up without embarking on this project using the AI as an aid.</strong></p>
            </div>
        </details>

        <!-- Isn't AI bad for the environment? Dropdown -->
<details class="main-dropdown">
    <summary class="dropdown-header">
        <h2>Isn't AI bad for the environment?</h2>
    </summary>
    <div class="dropdown-content">
        <p>The largest and most well-known AI models created by private corporations currently require a significant amount of energy to train. So does running social media platforms‚Äîin fact, a social media interaction on Instagram or YouTube consumes significantly more energy than a ChatGPT AI prompt at the time of writing (November 2025).</p>

        <!-- Kimi K2 output - special formatting -->
        <div class="ai-output-box">
            <h4>Kimi K2 output:</h4>
            <p><strong>AI tasks are remarkably efficient:</strong></p>
            <ul>
                <li>ChatGPT text generation: ~0.3 watt-hours per prompt (source: Muckypaws)</li>
                <li>Image generation (DALL-E): ~2.9 Wh per image</li>
                <li>Audio generation: ~5 Wh per minute</li>
            </ul>
            <p><strong>Social media consumes far more per interaction:</strong></p>
            <ul>
                <li>TikTok: ~10.4 Wh per 15-second video (source: Muckypaws)</li>
                <li>YouTube HD: ~12 Wh per 5 minutes of streaming</li>
                <li>Instagram/Facebook browsing: ~3.3-5.5 Wh per scroll/post (source: Muckypaws)</li>
            </ul>
            <p>A key nuance: <strong>training large AI models is extremely energy-intensive</strong> (millions of kWh), but this is a one-time cost amortized over billions of uses. Inference (actual usage) is very efficient. As one analysis notes: "attributing the enormous cost of training across all subsequent uses is like blaming the energy required to build a power station on every lightbulb it powers."</p>
            <p><strong>For typical queries, Kimi K2 is demonstrably 85-90% more energy-efficient</strong> than major proprietary models due to its architectural design (<a href="https://fastgptplus.com/en/posts/kimi-k2-vs-gpt-5 " target="_blank">FastGPTPlus</a>). DeepSeek's efficiency remains unverified, with credible sources suggesting it may actually consume more energy per complex task despite marketing claims of 40√ó savings . The ultimate efficiency advantage of open-source models comes not just from architecture, but from enabling smaller-scale, localized deployments that avoid the massive, always-on infrastructure of centralized platforms (<a href="https://www.linkedin.com/posts/jlishawa_ai-energyefficiency-sustainability-activity-7392891215240806400-yql0/ " target="_blank">LinkedIn</a>).</p>
            
            <hr>
            <h4>Sources</h4>
            <ol>
                <li><a href="https://seranking.com/blog/social-media-traffic-research-study/ " target="_blank">SE Ranking: Social Media Traffic Research Study</a></li>
                <li><a href="https://muckypaws.com/2025/04/21/is-ai-really-the-energy-villain/ " target="_blank">Muckypaws: Is AI Really the Energy Villain?</a></li>
                <li><a href="https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks " target="_blank">IEA: Data Centres and Data Transmission Networks</a></li>
            </ol>
        </div>

        <p>Note: The above output is from responses to several different prompts. Kimi did not recommend itself unsolicited; I specifically asked how Kimi and Deepseek compare to the largest and most well-known AI LLM models on energy usage.</p>
		<p><strong>What about water usage?</strong></p>
		
		<div class="ai-output-box">
		<!-- Dr. Ogadinma Kingsley Okakpu presentation content moved here -->
            <h4>Dr. Ogadinma Kingsley Okakpu's presentation, "AI Transition: the New Industrial Revolution":</h4>
            <ul>
                <li>AI (Data Centers) Global annual water consumption for cooling will reach 4.2 billion and 6.6 billion cubic meters annually by 2027 (University of Illinois report 2024).</li>
                <li>This is a major number, but important to note, this is a fraction of industrial agriculture use which uses 70 % of global freshwater with feed counting for a staggering 41% of all water used in Agriculture (Bryant Research 2025).</li>
                <li>Per day production of beef, leather, and other cow byproducts equate to over 5 trillion liters of water. Dairy production alone uses 4.6 trillion liters of water. ChatGPT in comparison uses 18.2 billion liters of water (Bryant Research 2025).</li>
                <li>Efforts to preserve global freshwater can't be hyperfocused on data center talking points, as agriculture remains exponentially a greater strain on water availability. Still, optimal usage of water for cooling data centers is a necessity for long-term usage.</li>
            </ul>
		</div>
		
	</div>
</details>

<!-- What about hallucinations and manipulation? Dropdown -->
<details class="main-dropdown">
    <summary class="dropdown-header">
        <h2>What about hallucinations?</h2>
    </summary>
    <div class="dropdown-content">
        <p>These are a serious concern. AIs have a tendency to respond in a confident and authoritative manner, even when they're hallucinating or using a bunk source. It's essential to have a critical eye for the statements they make, and to review the sources they provide or ask for additional ones.</p> 
		<p>I once had an AI summarize monthly transportation costs in the US, and the figure it provided was orders of magnitude away from the real figure. My knowledge of material conditions in the real world set off a warning bell that this information was likely wrong‚Äîand when I pressed the AI about the dollar amount it provided, it admitted that it was wrong and corrected itself with properly cited figures.</p>

        <p>LLMs also have a <a href="https://doublekunzite.github.io/what-can-ai-do/compare.html#model-agreeability">tendency to be excessively agreeable</a>. There are some rare cases of them goading people to take very harmful actions.</p>

        <!-- Kimi K2 output - special formatting -->
        <div class="ai-output-box">
            <h4>Kimi K2 output [6 December 2025]:</h4>
            <p><strong>What Are AI Hallucinations?</strong></p>
            <p>AI hallucinations occur when tools like ChatGPT generate false or made-up information that appears factual. Unlike humans who understand what they know, AI models work by predicting which words most likely come next based on patterns in their training data. When they don't have enough information, they don't say "I don't know"‚Äîinstead, they fill in the gaps with plausible-sounding but fabricated content.</p>
            
            <p><strong>Common examples include:</strong></p>
            <ul>
                <li><strong>Factual mistakes:</strong> Claiming the Eiffel Tower is 335 meters tall instead of 330 meters</li>
                <li><strong>Fabricated details:</strong> Making up quotes, references, or historical events that never happened</li>
                <li><strong>Contradictory answers:</strong> Giving different responses to the same question in the same conversation</li>
                <li><strong>Nonsensical statements:</strong> Providing answers that sound professional but make no sense</li>
            </ul>
            
            <p><strong>Why Do AI Systems Hallucinate?</strong></p>
            <p><strong>1. Gaps in Training Data:</strong> AI models learn from massive amounts of internet text, but this data has problems: outdated information, missing facts for uncommon topics, and errors in the data itself that the AI learns and repeats.</p>
            
            <p><strong>2. How AI Actually Works:</strong> These systems don't "know" facts‚Äîthey recognize patterns. They predict what word should come next based on probability, not truth. Without a built-in fact-checker, they can't tell if they're generating real information or just something that sounds right.</p>
            
            <p><strong>3. Memory and Context Limits:</strong> AI models can only "remember" a limited amount of conversation at once (called a context window). In long conversations, they may forget important details you mentioned earlier and make things up based on partial information.</p>
            
            <p><strong>4. Unclear Questions:</strong> When your prompt is vague or ambiguous, the AI has to guess what you want instead of asking for clarification like a human would. This often leads to hallucinations as it tries to fill in the missing pieces.</p>
            
            <p><strong>Why This Matters: The Risks for You</strong></p>
            <p><strong>Misinformation in Your Work:</strong> Using AI-generated content without checking it can spread false information, damaging your credibility at work or school. In important fields like healthcare or law, this can have serious consequences.</p>
            
            <p><strong>Real-World Problems:</strong></p>
            <ul>
                <li>A lawyer using AI for research discovered it cited non-existent court cases</li>
                <li>Someone seeking health advice could receive dangerous, hallucinated medical recommendations</li>
                <li>Students submitting AI-generated essays with fabricated sources face academic consequences</li>
            </ul>
            
            <p><strong>What You Can Do: Practical Solutions</strong></p>
            <p><strong>1. Ask Better Questions (Prompt Engineering):</strong> Be specific and clear in your requests. Instead of "Tell me about climate change," ask "What are three scientifically-supported effects of climate change on Arctic ice, with sources?" Provide context and use examples.</p>
            
            <p><strong>2. Check the AI's Work‚ÄîAlways:</strong> Treat AI as a helpful assistant, not an infallible expert. Verify facts using trusted sources, look up any statistics, quotes, or references the AI provides, and cross-check important information with at least one other reliable source.</p>
            
            <p><strong>3. Use AI Tools That Access Current Information:</strong> Some AI applications can search the internet or connect to databases in real-time. These tools are less likely to hallucinate because they can pull actual facts rather than relying only on their training data.</p>
            
            <p><strong>4. Adjust the Settings for Accuracy:</strong> If your AI tool allows it, lower the "temperature" to make responses more focused and predictable (less creative guessing), and adjust the "top-p" parameter to limit how many word choices the AI considers.</p>
            
            <p><strong>5. Break Down Complex Questions:</strong> Instead of asking one big question, split it into smaller parts: ask for definitions first, then examples, then connections between ideas.</p>
            
            <p><strong>6. Provide Feedback:</strong> Many AI systems learn from corrections. Use thumbs-up/down buttons when available, and tell the AI when it's wrong: "Actually, that's incorrect‚Äîthe correct information is..."</p>
            
            <p><strong>7. Know When to Trust (and Not Trust) AI:</strong></p>
            <ul>
                <li><strong>Good uses:</strong> Brainstorming ideas, drafting emails or outlines, explaining general concepts, coding assistance (with review)</li>
                <li><strong>Be extra careful with:</strong> Specific facts, dates, and figures; medical, legal, or financial advice; recent events; citing sources</li>
            </ul>
            
            <p><strong>Quick Checklist Before Using AI Information:</strong></p>
            <ul>
                <li>Did I ask a clear, specific question?</li>
                <li>Is this information critical or could errors cause harm?</li>
                <li>Have I verified important facts with reliable sources?</li>
                <li>Does the AI cite sources I can check?</li>
                <li>Am I using a tool with real-time information access when needed?</li>
            </ul>
            
            <p><strong>The Bottom Line:</strong> AI hallucinations are a built-in limitation of current technology, not a bug that will disappear soon. By understanding why they happen and following these practical steps, you can enjoy the benefits of AI while protecting yourself from its mistakes. The key is healthy skepticism‚Äîlet AI help you work faster and brainstorm better, but always verify critical information before acting on it.</p>
            
            <hr>
            <h4>Sources</h4>
            <ol>
                <li><a href="https://www.grammarly.com/blog/ai/what-are-ai-hallucinations/" target="_blank">Grammarly: "What Are AI Hallucinations?"</a></li>
                <li><a href="https://nexla.com/ai-infrastructure/llm-hallucination/" target="_blank">Nexla: "LLM Hallucination‚ÄîTypes, Causes, and Solutions"</a></li>
                <li><a href="https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models" target="_blank">Lakera: "Guide to Hallucinations in Large Language Models"</a></li>
                <li><a href="https://www.k2view.com/blog/llm-hallucination/" target="_blank">k2view: "LLM Hallucination Risks and Prevention"</a></li>
                <li><a href="https://medium.com/@nirdiamant21/llm-hallucinations-explained-8c76cdd82532" target="_blank">Medium: "LLM Hallucinations Explained"</a></li>
            </ol>
        </div>
    </div>
</details>

        <footer>
            <p><a href="index.html">‚Üê Back to Home</a></p>
        </footer>
    </main>
	<script src="/what-can-ai-do/js/theme.js"></script>
    <script src="/what-can-ai-do/js/navigation.js"></script>
</body>
</html>